@inproceedings{Mariko-fincausal-2020,
	title ={{The Financial Document Causality Detection Shared Task (FinCausal 2020)}}, author = {Mariko, Dominique and Abi Akl, Hanna and Labidurie, Estelle and Durfort, Stephane and de Mazancourt, Hugues and El-Haj, Mahmoud},
	booktitle ={{The 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020}},
	year = {2020},
	address = {Barcelona, Spain} }

@inproceedings{renersans,
	title={RENERSANs: Relation Extraction and Named Entity Recognition as Sequence Annotation},
	author={Davletov, Adis and Gordeev, Denis and Rey, Alexey and Arefyev, Nikolay},
	booktitle={Computational Linguistics and Intellectual Technologies},
	pages={187--197},
	year={2020}
}

Gordeev,  D.,  Davletov,  A.,  Rey,  A.,  Akzhigitova,  G.,  Geymbukh,  G.:  Relationextraction dataset for the russian. In: Proceedings of Dialogue (2020)

@inproceedings{bertoftrades,
	title={BERT of all trades, master of some},
	author={Gordeev, Denis and Lykova, Olga},
	booktitle={Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying},
	pages={93--98},
	year={2020}
}

@article{dai2020kungfupanda,
	title={Kungfupanda at SemEval-2020 Task 12: BERT-Based Multi-Task Learning for Offensive Language Detection},
	author={Dai, Wenliang and Yu, Tiezheng and Liu, Zihan and Fung, Pascale},
	journal={arXiv preprint arXiv:2004.13432},
	year={2020}
}


@article{ruder2017overview,
	title={An overview of multi-task learning in deep neural networks},
	author={Ruder, Sebastian},
	journal={arXiv preprint arXiv:1706.05098},
	year={2017}
}


@article{ghaddar2018robust,
	title={Robust lexical features for improved neural network named-entity recognition},
	author={Ghaddar, Abbas and Langlais, Philippe},
	journal={arXiv preprint arXiv:1806.03489},
	year={2018}
}


@article{li2019causality,
	title={Causality Extraction based on Self-Attentive BiLSTM-CRF with Transferred Embeddings},
	author={Li, Zhaoning and Li, Qi and Zou, Xiaotian and Ren, Jiangtao},
	journal={arXiv preprint arXiv:1904.07629},
	year={2019}
}


@article{khoo,
	author = {Khoo, Christopher S. G. And Kornfilt, Jaklin And Oddy, Robert N. And Myaeng, Sung Hyon},
	title = "{Automatic Extraction of Cause-Effect Information from Newspaper Text Without Knowledge-based Inferencing}",
	journal = {Literary and Linguistic Computing},
	volume = {13},
	number = {4},
	pages = {177-186},
	year = {1998},
	month = {12},
	abstract = "{This study investigated how effectively cause-effect information can be extracted from newspaper text using a simple computational method (i.e. without knowledge-based inferencing and without full parsing of sentences). An automatic method was developed for identifying and extracting cause-effect information in Wall Street Journal text using linguistic clues and pattern matching. The set of linguistic patterns used for identifying causal relationships was based on a through review of the literature and on an analysis of sample sentences from the Wall Street Journal. The cause-effect information extracted using the method was compared with that identified by two human judges. The program successfully extracted ˜68\\% of the causal relationships identified by both judges (the intersection of the two sets of causal relationships identified by the judges.) Of the instances that the computer program identified as causal relationships, ˜25\\% were identified by both judges, and 64\\% were identified by at least one of the judges. Problems encountered are discussed.}",
	issn = {0268-1145},
	doi = {10.1093/llc/13.4.177},
	url = {https://doi.org/10.1093/llc/13.4.177},
	eprint = {https://academic.oup.com/dsh/article-pdf/13/4/177/10888761/177.pdf},
}


@article{asghar2016automatic,
	title={Automatic extraction of causal relations from natural language texts: a comprehensive survey},
	author={Asghar, Nabiha},
	journal={arXiv preprint arXiv:1605.07895},
	year={2016}
}


@inproceedings{navigli2010annotated,
  title={An Annotated Dataset for Extracting Definitions and Hypernyms from the Web.},
  author={Navigli, Roberto and Velardi, Paola and Ruiz-Mart{\'\i}nez, Juana Mar{\'\i}a and others},
  booktitle={LREC},
  year={2010}
}
@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}
@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}
@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}
@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}
@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}
@inproceedings{borsch2011,
    title = "A Particle Filter algorithm for {B}ayesian Word segmentation",
    author = {B{\"o}rschinger, Benjamin  and
      Johnson, Mark},
    booktitle = "Proceedings of the Australasian Language Technology Association Workshop 2011",
    month = dec,
    year = "2011",
    address = "Canberra, Australia",
    url = "https://www.aclweb.org/anthology/U11-1004",
    pages = "10--18",
}
@article{ACM:83,
	author = {Association for Computing Machinery},
	year = "1983",
	journal = {Computing Reviews},
	volume = "24",
	number = "11",
	pages = "503--512",
}
@inproceedings{spala-etal-2019-deft,
    title = "{DEFT}: A corpus for definition extraction in free- and semi-structured text",
    author = "Spala, Sasha  and
      Miller, Nicholas A.  and
      Yang, Yiming  and
      Dernoncourt, Franck  and
      Dockhorn, Carl",
    booktitle = "Proceedings of the 13th Linguistic Annotation Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4015",
    pages = "124--131",
}
@inproceedings{jin-etal-2013-mining,
    title = "Mining Scientific Terms and their Definitions: A Study of the {ACL} Anthology",
    author = "Jin, Yiping  and
      Kan, Min-Yen  and
      Ng, Jun-Ping  and
      He, Xiangnan",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1073",
    pages = "780--790",
}
@article{Wolf2019HuggingFacesTS,
author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R'emi and Funtowicz, Morgan and Brew, Jamie},
journal = {ArXiv},
title = {{HuggingFace's Transformers: State-of-the-art Natural Language Processing}},
volume = {abs/1910.0},
year = {2019}
}
@article{roberta,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - Unknown - Roberta A robustly optimized bert pretraining approach.pdf:pdf},
journal = {arxiv.org},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {https://arxiv.org/abs/1907.11692 http://arxiv.org/abs/1907.11692},
year = {2019}
}
@inproceedings{Heist2017,
abstract = {Large-scale knowledge graphs, such as DBpedia, Wikidata, or YAGO, can be enhanced by relation extraction from text, using the data in the knowledge graph as training data, i.e., using distant supervision. While most existing approaches use language-specific methods (usually for English), we present a language-agnostic approach that exploits background knowledge from the graph instead of language-specific techniques and builds machine learning models only from language-independent features. We demonstrate the extraction of relations from Wikipedia abstracts, using the twelve largest language editions of Wikipedia. From those, we can extract 1.6M new relations in DBpedia at a level of precision of 95{\%}, using a RandomForest classifier trained only on language-independent features. Furthermore, we show an exemplary geographical breakdown of the information extracted.},
author = {Heist, Nicolas and Paulheim, Heiko},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-68288-4_23},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heist, Conference, 2017 - Unknown - Language-agnostic relation extraction from wikipedia abstracts.pdf:pdf},
isbn = {9783319682877},
issn = {16113349},
pages = {383--399},
title = {{Language-agnostic relation extraction from wikipedia abstracts}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-68288-4{\_}23},
volume = {10587 LNCS},
year = {2017}
}
@inproceedings{BaldiniSoares2019,
abstract = {General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED.},
archivePrefix = {arXiv},
arxivId = {1906.03158},
author = {{Baldini Soares}, Livio and FitzGerald, Nicholas and Ling, Jeffrey and Kwiatkowski, Tom},
booktitle = {arxiv.org},
doi = {10.18653/v1/p19-1279},
eprint = {1906.03158},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldini Soares et al. - 2019 - Matching the Blanks Distributional Similarity for Relation Learning.pdf:pdf},
pages = {2895--2905},
title = {{Matching the Blanks: Distributional Similarity for Relation Learning}},
url = {https://arxiv.org/abs/1906.03158},
year = {2019}
}
@inproceedings{attention,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Adv. Neural Inf. Process. Syst.},
eprint = {1706.03762},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention is all you need.pdf:pdf},
issn = {10495258},
pages = {5999--6009},
title = {{Attention is all you need}},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need},
volume = {2017-Decem},
year = {2017}
}
@book{Pustejovsky2013,
abstract = {Create your own natural language training corpus for machine learning. Whether you're working with English, Chinese, or any other natural language, this hands-on book guides you through a proven annotation development cycle—the process of adding metadata to your training corpus to help ML algorithms work more efficiently. You don't need any programming or linguistics experience to get started. Using detailed examples at every step, you'll learn how the MATTER Annotation Development Process helps you Model, Annotate, Train, Test, Evaluate, and Revise your training corpus. You also get a complete walkthrough of a real-world annotation project. Define a clear annotation goal before collecting your dataset (corpus) Learn tools for analyzing the linguistic content of your corpus Build a model and specification for your annotation project Examine the different annotation formats, from basic XML to the Linguistic Annotation Framework Create a gold standard corpus that can be used to train and test ML algorithms Select the ML algorithms that will process your annotated data Evaluate the test results and revise your annotation task Learn how to use lightweight software for annotating texts and adjudicating the annotations This book is a perfect companion to O'Reilly's Natural Language Processing with Python.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Pustejovsky, James and Stubbs, Amber},
booktitle = {Vasa},
doi = {1332788036},
eprint = {arXiv:1011.1669v3},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pustejovsky, Stubbs - 2013 - Natural language annotation for machine learning.pdf:pdf},
isbn = {9781449306663},
issn = {1098-6596},
pages = {1--97},
pmid = {25246403},
title = {{Natural language annotation for machine learning}},
url = {www.it-ebooks.info http://it-ebooks.info/book/681/{\%}5Cnpapers3://publication/uuid/906A922E-DE39-4CAB-8067-F222D065ACEF},
year = {2013}
}
@inproceedings{Le2019,
author = {Le, T A and Petrov, M A and Kurato, Y. M. and Burtsev, M S},
booktitle = {Comput. Linguist. Intellect. Technol. Pap. from Annu. Int. Conf. “Dialogue”},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le et al. - 2019 - Sentence Level Representation and Language Models in The Task of Coreference Resolution for Russian.pdf:pdf},
pages = {341--350},
title = {{Sentence Level Representation and Language Models in The Task of Coreference Resolution for Russian}},
year = {2019}
}
@inproceedings{tacred,
abstract = {Organized relational knowledge in the form of “knowledge graphs” is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset, obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2{\%} to 26.7{\%}.},
author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D},
booktitle = {EMNLP 2017 - Conf. Empir. Methods Nat. Lang. Process. Proc.},
doi = {10.18653/v1/d17-1004},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Position-aware attention and supervised data improve slot filling.pdf:pdf},
isbn = {9781945626838},
pages = {35--45},
title = {{Position-aware attention and supervised data improve slot filling}},
year = {2017}
}
@techreport{Zhang,
abstract = {Organized relational knowledge in the form of "knowledge graphs" is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset, obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F 1 score increases markedly from 22.2{\%} to 26.7{\%}.},
author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Position-aware Attention and Supervised Data Improve Slot Filling.pdf:pdf},
pages = {35--45},
title = {{Position-aware Attention and Supervised Data Improve Slot Filling}}
}
@techreport{spanbert,
archivePrefix = {arXiv},
arxivId = {1907.10529v3},
author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer and Allen, †},
eprint = {1907.10529v3},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joshi et al. - Unknown - SpanBERT Improving Pre-training by Representing and Predicting Spans.pdf:pdf},
title = {{SpanBERT: Improving Pre-training by Representing and Predicting Spans}},
url = {https://github.com/facebookresearch/},
year = {2019}
}
@article{bert,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@article{ontonotes,
abstract = {A forged and round-rolled pure tantalum bar stock was observed to exhibit large asymmetry in bulk plastic flow response when subjected to large strain Taylor cylinder impact testing. This low-symmetry behavior was analyzed experimentally investigating both the initial stock and the impact-deformed material via x-ray crystallographic texture measurements and automated electron back-scatter diffraction scans to establish spatial microstructural uniformity. Polycrystal simulations based upon the (I 10) measured duplex texture and experimentally inferred deformation mechanisms were performed to project discrete yield surface shapes. Subsequent least squares fitting and eigensystem analysis of the resulting quadratic fourth-order tensors revealed strong normal/shear stress coupling in the yield surface shape. This mixed-mode coupling produces a shearing deformation in the 1-2 impact plane of a Taylor specimen whose axis is coincident with the compressive 3-axis. The resultant deformation generates an unusual rectangular-shaped impact footprint that is confirmed by finite-element calculations compared to experimental post-test geometries. (C) 2002 Elsevier Science Ltd. All rights reserved.},
author = {et Al, Ralph Weischedel},
isbn = {0749-6419},
journal = {Linguist. Data Consort.},
title = {{OntoNotes Release 5.0 LDC2013T19}},
year = {2013}
}
@inproceedings{freebase,
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Free-base currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
booktitle = {Proc. ACM SIGMOD Int. Conf. Manag. Data},
doi = {10.1145/1376616.1376746},
isbn = {9781605581026},
issn = {07308078},
keywords = {Design,Human factors,Languages},
pages = {1247--1249},
title = {{Freebase: A collaboratively created graph database for structuring human knowledge}},
year = {2008}
}
@inproceedings{factrueval16,
abstract = {In this paper, we describe the rules and results of the FactRuEval information extraction competition held in 2016 as part of the Dialogue Evaluation initiative in the run-up to Dialogue 2016. The systems were to extract information from Russian texts and competed in two named entity extraction tracks and one fact extraction track. The paper describes the tasks set before the participants and presents the scores achieved by the contending systems. Additionally, we dwell upon the scoring methods employed for evaluating the results of all the three tracks and provide some preliminary analysis of the state of the art in Information Extraction for Russian texts. We also provide a detailed description of the composition and general organization of the annotated corpus created for the competition by volunteers using the OpenCorpora.org platform. The corpus is publicly available and is expected to evolve in the future.},
author = {Starostin, A. S. and Bocharov, V. V. and Alexeeva, S. V. and Bodrova, A. A. and Chuchunkov, A. S. and Dzhumaev, S. S. and Efimenko, I. V. and Granovsky, D. V. and Khoroshevsky, V. F. and Krylova, I. V. and Nikolaeva, M. A. and Smurov, I. M. and Toldova, S. Y.},
booktitle = {Komp'juternaja Lingvistika i Intellektual'nye Tehnol.},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Starostin et al. - 2016 - FactRuEval 2016 Evaluation of named entity recognition and fact extraction systems for Russian.pdf:pdf},
issn = {20757182},
keywords = {Evaluation,Fact extraction,Information extraction,Named entity recognition,Relation extraction},
pages = {702--720},
title = {{FactRuEval 2016: Evaluation of named entity recognition and fact extraction systems for Russian}},
year = {2016}
}
@inproceedings{nyt10,
abstract = {Several recent works on relation extraction have been applying the distant supervision paradigm: instead of relying on annotated text to learn how to predict relations, they employ existing knowledge bases (KBs) as source of supervision. Crucially, these approaches are trained based on the assumption that each sentence which mentions the two related entities is an expression of the given relation. Here we argue that this leads to noisy patterns that hurt precision, in particular if the knowledge base is not directly related to the text we are working with. We present a novel approach to distant supervision that can alleviate this problem based on the following two ideas: First, we use a factor graph to explicitly model the decision whether two entities are related, and the decision whether this relation is mentioned in a given sentence; second, we apply constraint-driven semi-supervision to train this model without any knowledge about which sentences express the relations in our training KB. We apply our approach to extract relations from the New York Times corpus and use Freebase as knowledge base. When compared to a state-of-the-art approach for relation extraction under distant supervision, we achieve 31{\%} error reduction. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
author = {Riedel, Sebastian and Yao, Limin and McCallum, Andrew},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-642-15939-8_10},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riedel, Yao, McCallum - 2010 - Modeling relations and their mentions without labeled text.pdf:pdf},
isbn = {3642159389},
issn = {03029743},
number = {PART 3},
pages = {148--163},
title = {{Modeling relations and their mentions without labeled text}},
volume = {6323 LNAI},
year = {2010}
}
@inproceedings{distant-supervision,
abstract = {Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE- style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of enti- ties that appears in some Freebase relation, we find all sentences containing those entities in a large un- labeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 re- lations at a precision of 67.6{\%}. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.},
author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
doi = {10.3115/1690219.1690287},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mintz et al. - 2009 - Distant supervision for relation extraction without labeled data.pdf:pdf},
pages = {1003--1011},
title = {{Distant supervision for relation extraction without labeled data}},
year = {2009}
}
@inproceedings{min-etal-2013-distant,
address = {Atlanta, Georgia},
author = {Min, Bonan and Grishman, Ralph and Wan, Li and Wang, Chang and Gondek, David},
booktitle = {Proc. 2013 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.},
pages = {777--782},
publisher = {Association for Computational Linguistics},
title = {{Distant Supervision for Relation Extraction with an Incomplete Knowledge Base}},
url = {https://www.aclweb.org/anthology/N13-1095},
year = {2013}
}
@article{distre,
abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6{\%} and 88.7{\%} F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6$\backslash${\%} F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE.},
address = {Florence, Italy},
archivePrefix = {arXiv},
arxivId = {1907.10529},
author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
eprint = {1907.10529},
journal = {Proc. 57th Annu. Meet. Assoc. Comput. Linguist.},
pages = {1388--1398},
publisher = {Association for Computational Linguistics},
title = {{SpanBERT: Improving Pre-training by Representing and Predicting Spans}},
url = {https://www.aclweb.org/anthology/P19-1134 http://arxiv.org/abs/1907.10529},
year = {2019}
}
@inproceedings{deeppavlov,
abstract = {Adoption of messaging communication and voice assistants has grown rapidly in the last years. This creates a demand for tools that speed up prototyping of featurerich dialogue systems. An open-source library DeepPavlov is tailored for development of conversational agents. The library prioritises efficiency, modularity, and extensibility with the goal to make it easier to develop dialogue systems from scratch and with limited data available. It supports modular as well as end-to-end approaches to implementation of conversational agents. Conversational agent consists of skills and every skill can be decomposed into components. Components are usually models which solve typical NLP tasks such as intent classification, named entity recognition or pre-trained word vectors. Sequence-to-sequence chit-chat skill, question answering skill or task-oriented skill can be assembled from components provided in the library.},
author = {Burtsev, Mikhail and Seliverstov, Alexander and Airapetyan, Rafael and Arkhipov, Mikhail and Baymurzina, Dilyara and Bushkov, Nickolay and Gureenkova, Olga and Khakhulin, Taras and Kuratov, Yuri and Kuznetsov, Denis and Litinsky, Alexey and Logacheva, Varvara and Lymar, Alexey and Malykh, Valentin and Petrov, Maxim and Polulyakh, Vadim and Pugachev, Leonid and Sorokin, Alexey and Vikhreva, Maria and Zaynutdinov, Marat},
booktitle = {ACL 2018 - 56th Annu. Meet. Assoc. Comput. Linguist. Proc. Syst. Demonstr.},
doi = {10.18653/v1/p18-4021},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burtsev et al. - 2018 - DeepPavlov Open-Source library for dialogue systems.pdf:pdf},
isbn = {9781948087650},
pages = {122--127},
title = {{DeepPavlov: Open-Source library for dialogue systems}},
url = {https://github.com/deepmipt/},
year = {2018}
}
@inproceedings{Modi:12,
address = {Montr{\'{e}}al, QC, Canada},
author = {Modi, Ashutosh and Titov, Ivan and Klementiev, Alexandre},
booktitle = {Proc. NAACL-HLT Work. Induction Linguist. Struct.},
pages = {1--7},
publisher = {Association for Computational Linguistics},
series = {WILS{\~{}}2012},
title = {{Unsupervised Induction of Frame-Semantic Representations}},
url = {https://aclweb.org/anthology/W12-1901},
year = {2012}
}
@inproceedings{kallmeyer2018coarse,
author = {Kallmeyer, Laura and QasemiZadeh, Behrang and Cheung, Jackie Chi Kit},
booktitle = {Proc. Seventh Jt. Conf. Lex. Comput. Semant.},
pages = {130--141},
title = {{Coarse Lexical Frame Acquisition at the Syntax--Semantics Interface Using a Latent-Variable PCFG Model}},
year = {2018}
}
@inproceedings{D18-1523,
author = {Amrami, Asaf and Goldberg, Yoav},
booktitle = {Proc. 2018 Conf. Empir. Methods Nat. Lang. Process.},
pages = {4860--4867},
publisher = {Association for Computational Linguistics},
title = {{Word Sense Induction with Neural biLM and Symmetric Patterns}},
url = {http://aclweb.org/anthology/D18-1523},
year = {2018}
}
@article{NIPS2017_7181,
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {1706.03762},
journal = {CoRR},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
volume = {abs/1706.0},
year = {2017}
}
@inproceedings{P18-2010,
abstract = {We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.},
address = {Melbourne, Australia},
author = {Ustalov, Dmitry and Panchenko, Alexander and Kutuzov, Andrey and Biemann, Chris and Ponzetto, Simone Paolo},
booktitle = {Proc. 56th Annu. Meet. Assoc. Comput. Linguist. (Volume 2 Short Pap.},
pages = {55--62},
publisher = {Association for Computational Linguistics},
title = {{Unsupervised Semantic Frame Induction using Triclustering}},
url = {https://www.aclweb.org/anthology/P18-2010},
year = {2018}
}
@article{DBLP:journals/corr/abs-1810-04805,
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
journal = {CoRR},
title = {{{\{}BERT:{\}} Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
volume = {abs/1810.0},
year = {2018}
}
@article{joshi2019spanbert,
author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
journal = {arXiv Prepr. arXiv1907.10529},
title = {{Spanbert: Improving pre-training by representing and predicting spans}},
year = {2019}
}
@inproceedings{struyansky_arefyev_18,
author = {Struyanskiy, Oleg and Arefyev, Nikolay},
booktitle = {Suppl. Proc. Seventh Int. Conf. Anal. Images, Soc. Networks Texts (AIST 2018)},
pages = {208--213},
title = {{Neural {\{}Networks{\}} with {\{}Attention{\}} for {\{}Word Sense Induction{\}}}},
url = {http://ceur-ws.org/Vol-2268/paper23.pdf},
year = {2018}
}
@inproceedings{Panchenko2012ASS,
author = {Panchenko, Alexander and Morozova, Olga and Naets, Hubert},
booktitle = {KONVENS},
title = {{A semantic similarity measure based on lexico-syntactic patterns}},
url = {https://pdfs.semanticscholar.org/c9f5/94202b5863d0303e32f7ee777b1c0cff5d1b.pdf?{\_}ga=2.136315699.2060908634.1550078587-109422695.1550078587},
year = {2012}
}
@inproceedings{Materna:13,
address = {Atlanta, GA, USA},
author = {Materna, Ji$\backslash$vr{\'{i}}},
booktitle = {Proc. 2013 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.},
pages = {482--486},
publisher = {Association for Computational Linguistics},
series = {NAACL-HLT{\~{}}2013},
title = {{Parameter Estimation for LDA-Frames}},
url = {https://aclweb.org/anthology/N13-1051},
year = {2013}
}
@inproceedings{zhang2017tacred,
author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D},
booktitle = {Proc. 2017 Conf. Empir. Methods Nat. Lang. Process. (EMNLP 2017)},
pages = {35--45},
title = {{Position-aware Attention and Supervised Data Improve Slot Filling}},
url = {https://nlp.stanford.edu/pubs/zhang2017tacred.pdf},
year = {2017}
}
@techreport{OConnor:13,
address = {Pittsburgh, PA, USA},
author = {O'Connor, Brendan},
institution = {Machine Learning Department, Carnegie Mellon University},
title = {{Learning Frames from Text with an Unsupervised Latent Variable Model}},
url = {https://arxiv.org/abs/1307.7382},
year = {2013}
}
@inproceedings{behrang_etal_19,
author = {QasemiZadeh, Behrang and Petruck, Miriam R L and Stodden, Regina and Kallmeyer, Laura and Candito, Marie},
booktitle = {Proc. 13th Int. Work. Semant. Eval.},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2019 Task 2: Unsupervised Lexical Frame Induction}},
year = {2019}
}
@inproceedings{brat,
address = {Avignon, France},
author = {Stenetorp, Pontus and Pyysalo, Sampo and Topi{\'{c}}, Goran and Ohta, Tomoko and Ananiadou, Sophia and Tsujii, Jun'ichi},
booktitle = {Proc. Demonstr. Sess. EACL 2012},
keywords = {brat},
mendeley-tags = {brat},
publisher = {Association for Computational Linguistics},
title = {{brat: a Web-based Tool for NLP-Assisted Text Annotation}},
year = {2012}
}
@inproceedings{panchenko_etal_18,
author = {Panchenko, Alexander and Lopukhina, Anastasiya and Ustalov, Dmitry and Lopukhin, Konstantin and Arefyev, Nikolay and Leontyev, Alexey and Loukachevitch, Natalia V},
booktitle = {Comput. Linguist. Intellect. Technol. Pap. from Annu. Int. Conf. “Dialogue”},
pages = {547--564},
publisher = {RSUH},
title = {{RUSSE'2018: {\{}A{\}} Shared Task on Word Sense Induction for the Russian Language}},
url = {http://arxiv.org/abs/1803.05795},
year = {2018}
}
@inproceedings{Cheung:13,
address = {Atlanta, GA, USA},
author = {Cheung, Jackie C K and Poon, Hoifung and Vanderwende, Lucy},
booktitle = {Proc. 2013 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.},
pages = {837--846},
publisher = {Association for Computational Linguistics},
series = {NAACL-HLT{\~{}}2013},
title = {{Probabilistic Frame Induction}},
url = {https://aclweb.org/anthology/N13-1104},
year = {2013}
}
@inproceedings{DBLP:journals/corr/abs-1805-09209,
author = {Arefyev, Nikolay and Ermolaev, Pavel and Panchenko, Alexander},
booktitle = {Comput. Linguist. Intellect. Technol. Pap. from Annu. Int. Conf. “Dialogue”},
pages = {68--84},
publisher = {RSUH},
title = {{How much does a word weigh? Weighting word embeddings for word sense induction}},
url = {http://arxiv.org/abs/1805.09209},
year = {2018}
}
@inproceedings{N18-1202,
author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {Proc. 2018 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. Vol. 1 (Long Pap.},
doi = {10.18653/v1/N18-1202},
pages = {2227--2237},
publisher = {Association for Computational Linguistics},
title = {{Deep Contextualized Word Representations}},
url = {http://aclweb.org/anthology/N18-1202},
year = {2018}
}
@inproceedings{S13-2050,
author = {Baskaya, Osman and Sert, Enis and Cirik, Volkan and Yuret, Deniz},
booktitle = {Second Jt. Conf. Lex. Comput. Semant. (*SEM), Vol. 2 Proc. Seventh Int. Work. Semant. Eval. (SemEval 2013)},
pages = {300--306},
publisher = {Association for Computational Linguistics},
title = {{AI-KU: Using Substitute Vectors and Co-Occurrence Modeling For Word Sense Induction and Disambiguation}},
url = {http://aclweb.org/anthology/S13-2050},
year = {2013}
}
@inproceedings{Kawahara:14,
address = {Baltimore, MD, USA},
author = {Kawahara, Daisuke and Peterson, Daniel W and Palmer, Martha},
booktitle = {Proc. 52nd Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap.},
pages = {1030--1040},
publisher = {Association for Computational Linguistics},
series = {ACL{\~{}}2014},
title = {{A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes}},
url = {https://aclweb.org/anthology/P14-1097},
year = {2014}
}
@inproceedings{K15-1026,
author = {Schwartz, Roy and Reichart, Roi and Rappoport, Ari},
booktitle = {Proc. Ninet. Conf. Comput. Nat. Lang. Learn.},
doi = {10.18653/v1/K15-1026},
pages = {258--267},
publisher = {Association for Computational Linguistics},
title = {{Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction}},
url = {http://aclweb.org/anthology/K15-1026},
year = {2015}
}
@inproceedings{S13-2049,
author = {Jurgens, David and Klapaftis, Ioannis},
booktitle = {Second Jt. Conf. Lex. Comput. Semant. (*SEM), Vol. 2 Proc. Seventh Int. Work. Semant. Eval. (SemEval 2013)},
pages = {290--299},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses}},
url = {http://aclweb.org/anthology/S13-2049},
year = {2013}
}
@inproceedings{C02-1114,
author = {Widdows, Dominic and Dorow, Beate},
booktitle = {COLING 2002 19th Int. Conf. Comput. Linguist.},
title = {{A Graph Model for Unsupervised Lexical Acquisition}},
url = {http://aclweb.org/anthology/C02-1114},
year = {2002}
}
@incollection{NIPS2017_7181_2,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Adv. Neural Inf. Process. Syst. 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
pages = {5998--6008},
publisher = {Curran Associates, Inc.},
title = {{Attention is {\{}All{\}} you {\{}Need{\}}}},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
year = {2017}
}
@inproceedings{Hearst1992,
abstract = {We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.},
annote = {Muito referenciado. Parece ter padr{\{}{\~{o}}{\}}es uteis.},
author = {Hearst, Marti A},
booktitle = {Proc. 14th Conf. Comput. Linguist.},
doi = {10.3115/992133.992154},
keywords = {semantic relations},
mendeley-tags = {semantic relations},
number = {July},
organization = {Association for Computational Linguistics Morristown, NJ, USA},
pages = {539--545},
publisher = {Association for Computational Linguistics},
series = {COLING '92},
title = {{Automatic acquisition of hyponyms from large text corpora}},
url = {http://portal.acm.org/citation.cfm?doid=992133.992154},
volume = {II},
year = {1992}
}
@article{Pedregosa2011,
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
pages = {2825--2830},
publisher = {JMLR. org},
title = {{Scikit-learn: Machine learning in Python}},
volume = {12},
year = {2011}
}
@inproceedings{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5754--5764},
  year={2019}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
