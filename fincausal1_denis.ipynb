{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fincausal1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aII-YRCC9HFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "123b3f0a-6879-471d-f440-33d5a55e6cbe"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkAH7sL1ASoG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d6ee906f-023c-40f3-a4f6-303d39accce1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtGPp58J91aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install transformers\n",
        "# !pip install seqeval\n",
        "# !sh setup.sh\n",
        "\n",
        "# !cp /content/drive/My\\ Drive/fincausal/* ./\n",
        "# # !cp /content/drive/My\\ Drive/trained-model-panic.zip ./\n",
        "# # !unzip trained-model-panic.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx1ki3w-OqEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d778e87-f516-4d6f-9fb5-9549f3831282"
      },
      "source": [
        "import json\n",
        "with open(f\"se_train.json\") as f:\n",
        "    data = json.load(f)\n",
        "len(set([d[\"text\"] for d in data]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17052"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkfNpVRaAcEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fdcc9eb-f17d-4711-b7db-9dd8298c4549"
      },
      "source": [
        "# --model_name_or_path bert-base-multilingual-uncased \\\n",
        "    # --do_train \\\n",
        "    # --do_eval \\\n",
        "!python run_classification.py \\\n",
        "    --data_dir ./ \\\n",
        "    --model_type roberta \\\n",
        "    --model_name_or_path roberta-base \\\n",
        "    --task_name panic \\\n",
        "    --output_dir trained-model \\\n",
        "    --max_seq_length 128 \\\n",
        "    --do_predict \\\n",
        "    --evaluate_during_training \\\n",
        "    --per_gpu_train_batch_size 48 \\\n",
        "    --per_gpu_eval_batch_size 48 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --fp16 \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-23 17:26:25.159598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "06/23/2020 17:26:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "06/23/2020 17:26:28 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "06/23/2020 17:26:28 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"panic\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "06/23/2020 17:26:28 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "06/23/2020 17:26:28 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "06/23/2020 17:26:28 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "06/23/2020 17:26:42 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "06/23/2020 17:26:42 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "06/23/2020 17:26:45 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_predict=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=True, fp16=True, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=2.0, output_dir='trained-model', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=48, per_gpu_train_batch_size=48, save_steps=500, seed=42, server_ip='', server_port='', task_name='panic', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "06/23/2020 17:26:45 - INFO - transformers.tokenization_utils -   Model name 'trained-model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'trained-model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "06/23/2020 17:26:45 - INFO - transformers.tokenization_utils -   Didn't find file trained-model/added_tokens.json. We won't load it.\n",
            "06/23/2020 17:26:45 - INFO - transformers.tokenization_utils -   loading file trained-model/vocab.json\n",
            "06/23/2020 17:26:45 - INFO - transformers.tokenization_utils -   loading file trained-model/merges.txt\n",
            "06/23/2020 17:26:45 - INFO - transformers.tokenization_utils -   loading file None\n",
            "06/23/2020 17:26:45 - INFO - transformers.tokenization_utils -   loading file trained-model/special_tokens_map.json\n",
            "06/23/2020 17:26:45 - INFO - transformers.tokenization_utils -   loading file trained-model/tokenizer_config.json\n",
            "06/23/2020 17:26:45 - INFO - __main__ -   Evaluate the following checkpoints: ['trained-model']\n",
            "06/23/2020 17:26:45 - INFO - transformers.configuration_utils -   loading configuration file trained-model/config.json\n",
            "06/23/2020 17:26:45 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"panic\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "06/23/2020 17:26:45 - INFO - transformers.modeling_utils -   loading weights file trained-model/pytorch_model.bin\n",
            "06/23/2020 17:26:50 - INFO - __main__ -   Creating features from dataset file at ./\n",
            "original 7386\n",
            "test 0\n",
            "06/23/2020 17:26:50 - INFO - panic_dataloader -   Writing example 0 of 7386\n",
            "06/23/2020 17:26:53 - INFO - __main__ -   Saving features into cached file ./cached_test_roberta-base_128_panic\n",
            "06/23/2020 17:26:54 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "06/23/2020 17:26:54 - INFO - __main__ -     Num examples = 7386\n",
            "06/23/2020 17:26:54 - INFO - __main__ -     Batch size = 48\n",
            "Evaluating: 100% 154/154 [00:27<00:00,  5.70it/s]\n",
            "preds [[ 3.1162896 -3.2348218]\n",
            " [ 3.6311324 -3.8036451]\n",
            " [ 3.5824394 -3.7614555]\n",
            " [ 3.4575787 -3.6453438]\n",
            " [ 3.3303053 -3.5347226]\n",
            " [ 3.442798  -3.6611118]\n",
            " [ 3.6364977 -3.8214116]\n",
            " [ 3.5700781 -3.7252312]\n",
            " [ 2.8540924 -2.9908273]\n",
            " [ 3.552139  -3.75136  ]]\n",
            "06/23/2020 17:27:21 - INFO - __main__ -   trained\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}